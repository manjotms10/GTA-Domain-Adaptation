{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import grad\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for project\n",
    "proj_root = \"/datasets/home/32/232/tdobhal/Project/\"\n",
    "\n",
    "# Root directory for dataset\n",
    "data_root = \"/datasets/home/32/232/tdobhal/Project/6_train/images/\"\n",
    "\n",
    "# Number of images in the directory\n",
    "num_images = 23418\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 16\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this size using a transformer.\n",
    "image_size = 256\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Number of training loops for critic/discriminator\n",
    "num_critic = 2\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.00005\n",
    "\n",
    "# Alpha hyperparam for RMS optimizers\n",
    "alpha = 0.9\n",
    "\n",
    "# Lambda hyperparam for gradient penalty\n",
    "lambda_gradient = 0.1\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = torch.cuda.device_count()\n",
    "\n",
    "# Device to run on\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Parameters:\n",
    "        \n",
    "        '''\n",
    "        self.device = device\n",
    "        self.data_path = data_root\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.train_names = glob.glob(self.data_path + 'real_A/*')\n",
    "        self.names = [self.train_names[i].split('/')[-1] for i in range(len(self.train_names))]\n",
    "        self.data_transforms = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.Resize(image_size),\n",
    "                torchvision.transforms.CenterCrop(image_size),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "    \n",
    "    def image_loader(self, image_name):\n",
    "        \"\"\"load image, returns cuda tensor\"\"\"\n",
    "        image = Image.open(image_name)\n",
    "        image = self.data_transforms(image).float()\n",
    "        image = torch.autograd.Variable(image, requires_grad=False)\n",
    "        image = image.unsqueeze(0)  # this is for VGG, may not be needed for ResNet\n",
    "        return image[0].to(self.device)  # assumes that you're using GPU\n",
    "\n",
    "    def show(self, img):\n",
    "        npimg = img.cpu().detach().numpy()\n",
    "        npimg = np.transpose(npimg, (1,2,0))\n",
    "        if npimg.shape[2] == 3:\n",
    "            plt.imshow(npimg)\n",
    "        else:\n",
    "            plt.imshow(npimg[:,:,0], cmap='gray')\n",
    "            \n",
    "    def imshow(self, img):\n",
    "        img = img / 2 + 0.5     # unnormalize\n",
    "        npimg = img.cpu().detach().numpy()\n",
    "        plt.figure(figsize = (10,2))\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)), aspect='auto')\n",
    "\n",
    "    def data_generator(self):\n",
    "        root = self.data_path\n",
    "        batch_size = self.batch_size\n",
    "        \n",
    "        images_dir = root + 'real_A/'\n",
    "        labels_dir = root + 'fake_B/'\n",
    "\n",
    "        while True:\n",
    "            x, y = [], []\n",
    "            idx = np.random.choice(self.names, batch_size)\n",
    "            for i in range(idx.shape[0]):\n",
    "                x.append(self.image_loader(images_dir + idx[i]))\n",
    "                y.append(self.image_loader(labels_dir + idx[i]))\n",
    "            yield torch.stack(x), torch.stack(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.01)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.01)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        # Convolution layers\n",
    "        \n",
    "        # input is (nc) x 256 x 256\n",
    "        self.conv1 = nn.Conv2d(nc, ngf, 4, 2, 1, bias=True)\n",
    "        self.lr1 = nn.LeakyReLU(inplace=True)\n",
    "        # state size. (ngf) x 128 x 128\n",
    "        self.conv2 = nn.Conv2d(ngf, ngf*2, 4, 2, 1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(ngf*2)\n",
    "        self.lr2 = nn.LeakyReLU(inplace=True)\n",
    "        # state size. (ngf*2) x 64 x 64\n",
    "        self.conv3 = nn.Conv2d(ngf*2, ngf*4, 4, 2, 1, bias=True)\n",
    "        self.bn3 = nn.BatchNorm2d(ngf*4)\n",
    "        self.lr3 = nn.LeakyReLU(inplace=True)\n",
    "        # state size. (ngf*4) x 32 x 32\n",
    "        self.conv4 = nn.Conv2d(ngf*4, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.bn4 = nn.BatchNorm2d(ngf*8)\n",
    "        self.lr4 = nn.LeakyReLU(inplace=True)\n",
    "        # state size. (ngf*8) x 16 x 16\n",
    "        self.conv5 = nn.Conv2d(ngf*8, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.bn5 = nn.BatchNorm2d(ngf*8)\n",
    "        self.lr5 = nn.LeakyReLU(inplace=True)\n",
    "        # state size. (ngf*8) x 8 x 8\n",
    "        self.conv6 = nn.Conv2d(ngf*8, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.bn6 = nn.BatchNorm2d(ngf*8)\n",
    "        self.lr6 = nn.LeakyReLU(inplace=True)\n",
    "        # state size. (ngf*8) x 4 x 4\n",
    "        self.conv7 = nn.Conv2d(ngf*8, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.bn7 = nn.BatchNorm2d(ngf*8)\n",
    "        self.lr7 = nn.LeakyReLU(inplace=True)\n",
    "        # state size. (ngf*8) x 2 x 2\n",
    "        self.conv8 = nn.Conv2d(ngf*8, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.bn8 = nn.BatchNorm2d(ngf*8)\n",
    "        self.r8 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Transpose Convolutional Layers\n",
    "        \n",
    "        # input is (ngf*8) x 1 x 1\n",
    "        self.tr_conv1 = nn.ConvTranspose2d(ngf*8, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.tr_bn1 = nn.BatchNorm2d(ngf*8)\n",
    "        self.tr_r1 = nn.ReLU(inplace=True)\n",
    "        # state size. (ngf*8)*2 x 2 x 2\n",
    "        self.tr_conv2 = nn.ConvTranspose2d((ngf*8)*2, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.tr_bn2 = nn.BatchNorm2d(ngf*8)\n",
    "        self.tr_r2 = nn.ReLU(inplace=True)\n",
    "        # state size. (ngf*8)*2 x 4 x 4\n",
    "        self.tr_conv3 = nn.ConvTranspose2d((ngf*8)*2, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.tr_bn3 = nn.BatchNorm2d(ngf*8)\n",
    "        self.tr_r3 = nn.ReLU(inplace=True)\n",
    "        # state size. (ngf*8)*2 x 8 x 8\n",
    "        self.tr_conv4 = nn.ConvTranspose2d((ngf*8)*2, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.tr_bn4 = nn.BatchNorm2d(ngf*8)\n",
    "        self.tr_r4 = nn.ReLU(inplace=True)\n",
    "        # state size. (ngf*8)*2 x 16 x 16\n",
    "        self.tr_conv5 = nn.ConvTranspose2d((ngf*8)*2, ngf*4, 4, 2, 1, bias=True)\n",
    "        self.tr_bn5 = nn.BatchNorm2d(ngf*4)\n",
    "        self.tr_r5 = nn.ReLU(inplace=True)\n",
    "        # state size. (ngf*4)*2 x 32 x 32\n",
    "        self.tr_conv6 = nn.ConvTranspose2d((ngf*4)*2, ngf*2, 4, 2, 1, bias=True)\n",
    "        self.tr_bn6 = nn.BatchNorm2d(ngf*2)\n",
    "        self.tr_r6 = nn.ReLU(inplace=True)\n",
    "        # state size. (ngf*2)*2 x 64 x 64\n",
    "        self.tr_conv7 = nn.ConvTranspose2d((ngf*2)*2, ngf, 4, 2, 1, bias=True)\n",
    "        self.tr_bn7 = nn.BatchNorm2d(ngf)\n",
    "        self.tr_r7 = nn.ReLU(inplace=True)\n",
    "        # state size. (ngf)*2 x 128 x 128\n",
    "        self.tr_conv8 = nn.ConvTranspose2d((ngf)*2, nc, 4, 2, 1, bias=True)\n",
    "        self.out = nn.Tanh()\n",
    "        # state size. (nc) x 256 x 256\n",
    "    \n",
    "    def forward(self, x):\n",
    "        c1 = self.conv1(x)\n",
    "        c2 = self.bn2(self.conv2(self.lr1(c1)))\n",
    "        c3 = self.bn3(self.conv3(self.lr2(c2)))\n",
    "        c4 = self.bn4(self.conv4(self.lr3(c3)))\n",
    "        c5 = self.bn5(self.conv5(self.lr4(c4)))\n",
    "        c6 = self.bn6(self.conv6(self.lr5(c5)))\n",
    "        c7 = self.bn7(self.conv7(self.lr6(c6)))\n",
    "        c8 = self.bn8(self.conv8(self.lr7(c7)))\n",
    "        \n",
    "        t1 = self.tr_bn1(self.tr_conv1(self.r8(c8)))\n",
    "        t1 = torch.cat((t1, c7), dim=1)\n",
    "        t2 = self.tr_bn2(self.tr_conv2(self.tr_r1(t1)))\n",
    "        t2 = torch.cat((t2, c6), dim=1)\n",
    "        t3 = self.tr_bn3(self.tr_conv3(self.tr_r2(t2)))\n",
    "        t3 = torch.cat((t3, c5), dim=1)\n",
    "        t4 = self.tr_bn4(self.tr_conv4(self.tr_r3(t3)))\n",
    "        t4 = torch.cat((t4, c4), dim=1)\n",
    "        t5 = self.tr_bn5(self.tr_conv5(self.tr_r4(t4)))\n",
    "        t5 = torch.cat((t5, c3), dim=1)\n",
    "        t6 = self.tr_bn6(self.tr_conv6(self.tr_r5(t5)))\n",
    "        t6 = torch.cat((t6, c2), dim=1)\n",
    "        t7 = self.tr_bn7(self.tr_conv7(self.tr_r6(t6)))\n",
    "        t7 = torch.cat((t7, c1), dim=1)\n",
    "        t8 = self.tr_conv8(self.tr_r7(t7))\n",
    "        t8 = self.out(t8)\n",
    "        return t8\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_a = Generator().to(device)\n",
    "gen_b = Generator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # input is (nc) x 256 x 256\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 128 x 128\n",
    "            nn.MaxPool2d((2, 2)), \n",
    "            \n",
    "            # state size. (ndf) x 64 x 64\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. 32 x 32\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "    \n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, ndf * 8, 4, 2, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        \n",
    "        self.flat = nn.Linear(ndf * 8 * 2 * 2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.flat(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_a = Discriminator().to(device)\n",
    "dis_b = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochTracker():\n",
    "    def __init__(self, in_file):\n",
    "        self.epoch = 0\n",
    "        self.iter = 0\n",
    "        self.in_file = in_file\n",
    "        self.file_exists = os.path.isfile(in_file)\n",
    "        if self.file_exists:\n",
    "            with open(in_file, 'r') as f: \n",
    "                d = f.read() \n",
    "                a, b = d.split(\";\")\n",
    "                self.epoch = int(a)\n",
    "                self.iter = int(b)\n",
    "     \n",
    "    def write(self, epoch, iteration):\n",
    "        self.epoch = epoch\n",
    "        self.iter = iteration\n",
    "        data = \"{};{}\".format(self.epoch, self.iter)\n",
    "        with open(self.in_file, 'w') as f:\n",
    "            f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (net): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (5): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (9): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (10): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (11): LeakyReLU(negative_slope=0.2, inplace)\n",
       "  )\n",
       "  (flat): Linear(in_features=2048, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataParallel for more than 1 gpu\n",
    "# gen_a = nn.DataParallel(gen_a, list(range(ngpu)))\n",
    "# dis_a = nn.DataParallel(dis_a, list(range(ngpu)))\n",
    "# gen_b = nn.DataParallel(gen_b, list(range(ngpu)))\n",
    "# dis_b = nn.DataParallel(dis_b, list(range(ngpu)))\n",
    "\n",
    "gen_a.apply(weights_init_normal)\n",
    "dis_a.apply(weights_init_normal)\n",
    "gen_b.apply(weights_init_normal)\n",
    "dis_b.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient penalty for Wasserstein Loss\n",
    "def gradient_penalty(real_dis, fake_dis, discriminator):\n",
    "    shape = [real_dis.size(0)] + [1] * (real_dis.dim() - 1)\n",
    "    alpha = torch.rand(shape).to(device)\n",
    "    z = real_dis + alpha * (fake_dis - real_dis)\n",
    "\n",
    "    # gradient penalty\n",
    "    z = Variable(z, requires_grad=True).to(device)\n",
    "    output = discriminator(z)\n",
    "    g = grad(output, z, grad_outputs=torch.ones(output.size()).to(device), create_graph=True)[0].view(z.size(0), -1)\n",
    "    gp = ((g.norm(p=2, dim=1) - 1)**2).mean()\n",
    "\n",
    "    return lambda_gradient * gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen_a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8969207fdbdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcriterion_pixel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL1Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moptim_gen_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moptim_gen_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptim_dis_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gen_a' is not defined"
     ]
    }
   ],
   "source": [
    "criterion_pixel = nn.L1Loss()\n",
    "\n",
    "optim_gen_a = torch.optim.RMSprop(gen_a.parameters(), lr, alpha)\n",
    "optim_gen_b = torch.optim.RMSprop(gen_b.parameters(), lr, alpha)\n",
    "optim_dis_a = torch.optim.RMSprop(dis_a.parameters(), lr, alpha)\n",
    "optim_dis_b = torch.optim.RMSprop(dis_b.parameters(), lr, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 376/1463] [D_A loss: 3.624161] [D_B loss: -0.582017] [G_A loss: 17.132257, G_B loss: 221.347229]\n",
      "[Epoch 0/100] [Batch 377/1463] [D_A loss: 2.647049] [D_B loss: 1.298769] [G_A loss: 25.230881, G_B loss: 175.617249]\n",
      "[Epoch 0/100] [Batch 378/1463] [D_A loss: -3.374094] [D_B loss: -0.396629] [G_A loss: 30.199238, G_B loss: -68.910294]\n",
      "[Epoch 0/100] [Batch 379/1463] [D_A loss: -1.817593] [D_B loss: -1.088959] [G_A loss: 40.297436, G_B loss: 4.145512]\n",
      "[Epoch 0/100] [Batch 380/1463] [D_A loss: -0.783661] [D_B loss: 0.596369] [G_A loss: 35.619064, G_B loss: -131.685089]\n",
      "[Epoch 0/100] [Batch 381/1463] [D_A loss: 5.895774] [D_B loss: 0.741749] [G_A loss: 30.809788, G_B loss: 92.037659]\n",
      "[Epoch 0/100] [Batch 382/1463] [D_A loss: -2.385512] [D_B loss: -0.034267] [G_A loss: 25.034981, G_B loss: 278.535614]\n",
      "[Epoch 0/100] [Batch 383/1463] [D_A loss: 2.875027] [D_B loss: -0.179871] [G_A loss: 22.917915, G_B loss: 302.011444]\n",
      "[Epoch 0/100] [Batch 384/1463] [D_A loss: 1.139215] [D_B loss: -0.719777] [G_A loss: 20.161165, G_B loss: 188.432053]\n",
      "[Epoch 0/100] [Batch 385/1463] [D_A loss: -2.969105] [D_B loss: 0.031504] [G_A loss: 22.540228, G_B loss: 61.327641]\n",
      "[Epoch 0/100] [Batch 386/1463] [D_A loss: 1.352526] [D_B loss: 0.274192] [G_A loss: 25.994459, G_B loss: 15.941800]\n",
      "[Epoch 0/100] [Batch 387/1463] [D_A loss: 2.142031] [D_B loss: 0.035893] [G_A loss: 27.521425, G_B loss: 98.553490]\n",
      "[Epoch 0/100] [Batch 388/1463] [D_A loss: 0.105393] [D_B loss: -0.114873] [G_A loss: 25.340658, G_B loss: 144.972992]\n",
      "[Epoch 0/100] [Batch 389/1463] [D_A loss: -1.862690] [D_B loss: 0.027220] [G_A loss: 22.910854, G_B loss: 202.605499]\n",
      "[Epoch 0/100] [Batch 390/1463] [D_A loss: 0.386035] [D_B loss: 0.135804] [G_A loss: 18.967461, G_B loss: 163.328049]\n",
      "[Epoch 0/100] [Batch 391/1463] [D_A loss: 2.070803] [D_B loss: 0.319953] [G_A loss: 14.015306, G_B loss: 140.418793]\n",
      "[Epoch 0/100] [Batch 392/1463] [D_A loss: 0.635418] [D_B loss: 0.007660] [G_A loss: 15.152356, G_B loss: 247.072647]\n",
      "[Epoch 0/100] [Batch 393/1463] [D_A loss: 3.017294] [D_B loss: -0.095266] [G_A loss: 16.150936, G_B loss: 139.304031]\n",
      "[Epoch 0/100] [Batch 394/1463] [D_A loss: 0.884478] [D_B loss: 0.165456] [G_A loss: 16.459471, G_B loss: 50.724831]\n",
      "[Epoch 0/100] [Batch 395/1463] [D_A loss: -1.292358] [D_B loss: 0.156519] [G_A loss: 16.629627, G_B loss: -32.257610]\n",
      "[Epoch 0/100] [Batch 396/1463] [D_A loss: 4.014480] [D_B loss: 0.395181] [G_A loss: 20.038792, G_B loss: -39.915348]\n",
      "[Epoch 0/100] [Batch 397/1463] [D_A loss: -0.621911] [D_B loss: -0.176425] [G_A loss: 22.474834, G_B loss: 162.165039]\n",
      "[Epoch 0/100] [Batch 398/1463] [D_A loss: -2.345724] [D_B loss: -0.009671] [G_A loss: 20.100880, G_B loss: 220.308670]\n",
      "[Epoch 0/100] [Batch 399/1463] [D_A loss: -0.859941] [D_B loss: 0.023109] [G_A loss: 17.841290, G_B loss: 208.349121]\n",
      "[Epoch 0/100] [Batch 400/1463] [D_A loss: -1.727193] [D_B loss: -0.071778] [G_A loss: 17.423939, G_B loss: 134.531418]\n",
      "[Epoch 0/100] [Batch 401/1463] [D_A loss: -1.243521] [D_B loss: -0.039339] [G_A loss: 19.519545, G_B loss: 4.166791]\n",
      "[Epoch 0/100] [Batch 402/1463] [D_A loss: -1.739396] [D_B loss: -0.178985] [G_A loss: 18.988468, G_B loss: -65.947800]\n",
      "[Epoch 0/100] [Batch 403/1463] [D_A loss: 3.917625] [D_B loss: -0.086216] [G_A loss: 20.431429, G_B loss: 61.289371]\n",
      "[Epoch 0/100] [Batch 404/1463] [D_A loss: -1.605983] [D_B loss: -0.086940] [G_A loss: 20.003708, G_B loss: 162.572769]\n",
      "[Epoch 0/100] [Batch 405/1463] [D_A loss: -1.584943] [D_B loss: 0.091051] [G_A loss: 17.805464, G_B loss: 135.033401]\n",
      "[Epoch 0/100] [Batch 406/1463] [D_A loss: -0.701281] [D_B loss: -0.015985] [G_A loss: 19.025537, G_B loss: 46.711273]\n",
      "[Epoch 0/100] [Batch 407/1463] [D_A loss: -1.281055] [D_B loss: -0.233193] [G_A loss: 20.085978, G_B loss: -62.314941]\n",
      "[Epoch 0/100] [Batch 408/1463] [D_A loss: -2.545002] [D_B loss: 0.065901] [G_A loss: 24.393425, G_B loss: -36.456562]\n",
      "[Epoch 0/100] [Batch 409/1463] [D_A loss: 3.825968] [D_B loss: -0.270551] [G_A loss: 26.745066, G_B loss: 80.286469]\n",
      "[Epoch 0/100] [Batch 410/1463] [D_A loss: -4.140132] [D_B loss: -0.469207] [G_A loss: 27.991949, G_B loss: 245.350983]\n",
      "[Epoch 0/100] [Batch 411/1463] [D_A loss: -3.903133] [D_B loss: 1.446141] [G_A loss: 25.999086, G_B loss: 227.758514]\n",
      "[Epoch 0/100] [Batch 412/1463] [D_A loss: -0.108165] [D_B loss: -0.178510] [G_A loss: 21.011000, G_B loss: 81.155556]\n",
      "[Epoch 0/100] [Batch 413/1463] [D_A loss: -4.006876] [D_B loss: -0.828730] [G_A loss: 19.963247, G_B loss: -36.624531]\n",
      "[Epoch 0/100] [Batch 414/1463] [D_A loss: -1.403131] [D_B loss: 0.205184] [G_A loss: 17.919664, G_B loss: -103.335915]\n",
      "[Epoch 0/100] [Batch 415/1463] [D_A loss: 5.279538] [D_B loss: 0.802609] [G_A loss: 20.940870, G_B loss: 68.181259]\n",
      "[Epoch 0/100] [Batch 416/1463] [D_A loss: -2.939113] [D_B loss: 0.245544] [G_A loss: 22.037590, G_B loss: 175.410934]\n",
      "[Epoch 0/100] [Batch 417/1463] [D_A loss: 1.905003] [D_B loss: -0.056290] [G_A loss: 22.462820, G_B loss: 148.208359]\n",
      "[Epoch 0/100] [Batch 418/1463] [D_A loss: -0.142541] [D_B loss: -0.505745] [G_A loss: 24.228861, G_B loss: 99.048164]\n",
      "[Epoch 0/100] [Batch 419/1463] [D_A loss: -0.576922] [D_B loss: 0.226918] [G_A loss: 22.774044, G_B loss: 49.188244]\n",
      "[Epoch 0/100] [Batch 420/1463] [D_A loss: -0.968555] [D_B loss: 0.163200] [G_A loss: 20.942692, G_B loss: 30.259232]\n",
      "[Epoch 0/100] [Batch 421/1463] [D_A loss: -0.333593] [D_B loss: 0.278831] [G_A loss: 18.775520, G_B loss: 166.938812]\n",
      "[Epoch 0/100] [Batch 422/1463] [D_A loss: -0.024237] [D_B loss: 0.422159] [G_A loss: 13.780824, G_B loss: 187.285019]\n",
      "[Epoch 0/100] [Batch 423/1463] [D_A loss: -1.339331] [D_B loss: -0.274252] [G_A loss: 11.438067, G_B loss: 82.247955]\n",
      "[Epoch 0/100] [Batch 424/1463] [D_A loss: -0.140273] [D_B loss: 0.271040] [G_A loss: 13.700537, G_B loss: 69.402100]\n",
      "[Epoch 0/100] [Batch 425/1463] [D_A loss: -0.939022] [D_B loss: -0.214108] [G_A loss: 16.782022, G_B loss: 133.871765]\n",
      "[Epoch 0/100] [Batch 426/1463] [D_A loss: -1.039626] [D_B loss: -0.345370] [G_A loss: 19.206341, G_B loss: 54.527512]\n",
      "[Epoch 0/100] [Batch 427/1463] [D_A loss: -0.800316] [D_B loss: 0.371813] [G_A loss: 21.203562, G_B loss: -67.439331]\n",
      "[Epoch 0/100] [Batch 428/1463] [D_A loss: 0.375843] [D_B loss: 0.027168] [G_A loss: 20.760551, G_B loss: -10.178836]\n",
      "[Epoch 0/100] [Batch 429/1463] [D_A loss: 0.338673] [D_B loss: -1.022411] [G_A loss: 19.668095, G_B loss: 78.260757]\n",
      "[Epoch 0/100] [Batch 430/1463] [D_A loss: -0.091136] [D_B loss: 0.098310] [G_A loss: 14.052825, G_B loss: 149.221878]\n",
      "[Epoch 0/100] [Batch 431/1463] [D_A loss: -1.442660] [D_B loss: 0.213443] [G_A loss: 14.730960, G_B loss: 217.845169]\n",
      "[Epoch 0/100] [Batch 432/1463] [D_A loss: -1.943190] [D_B loss: -0.086200] [G_A loss: 14.665655, G_B loss: 154.571762]\n",
      "[Epoch 0/100] [Batch 433/1463] [D_A loss: -1.562143] [D_B loss: -0.342367] [G_A loss: 14.211032, G_B loss: 25.412010]\n",
      "[Epoch 0/100] [Batch 434/1463] [D_A loss: -1.820001] [D_B loss: -0.256960] [G_A loss: 13.262753, G_B loss: -1.552130]\n",
      "[Epoch 0/100] [Batch 435/1463] [D_A loss: 1.471746] [D_B loss: 0.077209] [G_A loss: 10.973354, G_B loss: 8.402480]\n",
      "[Epoch 0/100] [Batch 436/1463] [D_A loss: 3.162470] [D_B loss: 0.025123] [G_A loss: 9.541302, G_B loss: 146.392715]\n",
      "[Epoch 0/100] [Batch 437/1463] [D_A loss: -1.056612] [D_B loss: -0.151483] [G_A loss: 8.965471, G_B loss: 183.162704]\n",
      "[Epoch 0/100] [Batch 438/1463] [D_A loss: 2.566098] [D_B loss: -0.340541] [G_A loss: 10.478768, G_B loss: 94.074745]\n",
      "[Epoch 0/100] [Batch 439/1463] [D_A loss: -0.581784] [D_B loss: 0.025434] [G_A loss: 12.659356, G_B loss: 54.712700]\n",
      "[Epoch 0/100] [Batch 440/1463] [D_A loss: -3.133877] [D_B loss: 0.185170] [G_A loss: 11.893699, G_B loss: -20.689234]\n",
      "[Epoch 0/100] [Batch 441/1463] [D_A loss: 0.917379] [D_B loss: 0.110484] [G_A loss: 12.666649, G_B loss: 129.217194]\n",
      "[Epoch 0/100] [Batch 442/1463] [D_A loss: -0.001512] [D_B loss: 0.223582] [G_A loss: 11.681976, G_B loss: 186.756287]\n",
      "[Epoch 0/100] [Batch 443/1463] [D_A loss: 0.463470] [D_B loss: 0.109516] [G_A loss: 11.892900, G_B loss: 123.418373]\n",
      "[Epoch 0/100] [Batch 444/1463] [D_A loss: -0.648224] [D_B loss: 0.046395] [G_A loss: 10.052781, G_B loss: 11.838560]\n",
      "[Epoch 0/100] [Batch 445/1463] [D_A loss: -0.362824] [D_B loss: 0.270475] [G_A loss: 12.232615, G_B loss: 92.057762]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 446/1463] [D_A loss: 0.503611] [D_B loss: -0.154540] [G_A loss: 13.466799, G_B loss: 54.934639]\n",
      "[Epoch 0/100] [Batch 447/1463] [D_A loss: -0.200527] [D_B loss: -0.009726] [G_A loss: 12.458622, G_B loss: 24.497839]\n",
      "[Epoch 0/100] [Batch 448/1463] [D_A loss: 0.321373] [D_B loss: 0.110224] [G_A loss: 11.490746, G_B loss: 7.014593]\n",
      "[Epoch 0/100] [Batch 449/1463] [D_A loss: 0.202745] [D_B loss: 0.033442] [G_A loss: 14.412921, G_B loss: 164.982269]\n",
      "[Epoch 0/100] [Batch 450/1463] [D_A loss: 0.526280] [D_B loss: -0.107980] [G_A loss: 15.332762, G_B loss: 114.682892]\n",
      "[Epoch 0/100] [Batch 451/1463] [D_A loss: -1.394155] [D_B loss: 0.089080] [G_A loss: 14.386254, G_B loss: 54.547249]\n",
      "[Epoch 0/100] [Batch 452/1463] [D_A loss: 0.616868] [D_B loss: -0.002745] [G_A loss: 15.211162, G_B loss: 118.713043]\n",
      "[Epoch 0/100] [Batch 453/1463] [D_A loss: -1.821199] [D_B loss: 0.043251] [G_A loss: 12.852713, G_B loss: -11.475356]\n",
      "[Epoch 0/100] [Batch 454/1463] [D_A loss: 1.126154] [D_B loss: -0.005827] [G_A loss: 11.577420, G_B loss: 59.338654]\n",
      "[Epoch 0/100] [Batch 455/1463] [D_A loss: -1.162246] [D_B loss: -0.132792] [G_A loss: 12.040671, G_B loss: 90.106552]\n",
      "[Epoch 0/100] [Batch 456/1463] [D_A loss: -4.254598] [D_B loss: -0.210723] [G_A loss: 13.356544, G_B loss: 284.972412]\n",
      "[Epoch 0/100] [Batch 457/1463] [D_A loss: -3.801251] [D_B loss: -0.117935] [G_A loss: 15.144284, G_B loss: 297.752625]\n",
      "[Epoch 0/100] [Batch 458/1463] [D_A loss: 9.647028] [D_B loss: 0.012043] [G_A loss: 16.027802, G_B loss: 103.755463]\n",
      "[Epoch 0/100] [Batch 459/1463] [D_A loss: -1.709594] [D_B loss: 0.262670] [G_A loss: 10.791020, G_B loss: -50.904919]\n",
      "[Epoch 0/100] [Batch 460/1463] [D_A loss: -2.294202] [D_B loss: 0.134527] [G_A loss: 7.452709, G_B loss: -115.680176]\n",
      "[Epoch 0/100] [Batch 461/1463] [D_A loss: 0.350612] [D_B loss: -0.072820] [G_A loss: 8.183114, G_B loss: -109.247688]\n",
      "[Epoch 0/100] [Batch 462/1463] [D_A loss: 4.839785] [D_B loss: 0.208076] [G_A loss: 9.507345, G_B loss: 91.272514]\n",
      "[Epoch 0/100] [Batch 463/1463] [D_A loss: -2.922672] [D_B loss: 0.128286] [G_A loss: 11.746535, G_B loss: 185.387985]\n",
      "[Epoch 0/100] [Batch 464/1463] [D_A loss: 0.875328] [D_B loss: 0.084351] [G_A loss: 12.642019, G_B loss: 203.330093]\n",
      "[Epoch 0/100] [Batch 465/1463] [D_A loss: -0.773561] [D_B loss: -0.033993] [G_A loss: 14.229845, G_B loss: 203.919632]\n",
      "[Epoch 0/100] [Batch 466/1463] [D_A loss: 1.217336] [D_B loss: -0.019856] [G_A loss: 15.063740, G_B loss: 163.585938]\n",
      "[Epoch 0/100] [Batch 467/1463] [D_A loss: -0.039125] [D_B loss: 0.304209] [G_A loss: 14.932011, G_B loss: 81.983849]\n",
      "[Epoch 0/100] [Batch 468/1463] [D_A loss: -0.204484] [D_B loss: 0.148267] [G_A loss: 12.713005, G_B loss: 52.698658]\n",
      "[Epoch 0/100] [Batch 469/1463] [D_A loss: -1.222378] [D_B loss: -0.184657] [G_A loss: 12.199476, G_B loss: -4.465140]\n",
      "[Epoch 0/100] [Batch 470/1463] [D_A loss: -1.232492] [D_B loss: 0.538450] [G_A loss: 13.409320, G_B loss: -36.825085]\n",
      "[Epoch 0/100] [Batch 471/1463] [D_A loss: -0.245779] [D_B loss: 0.087940] [G_A loss: 15.048400, G_B loss: 78.361931]\n",
      "[Epoch 0/100] [Batch 472/1463] [D_A loss: -0.008202] [D_B loss: 0.006203] [G_A loss: 16.199327, G_B loss: 177.967178]\n",
      "[Epoch 0/100] [Batch 473/1463] [D_A loss: 1.467036] [D_B loss: 0.031523] [G_A loss: 13.566535, G_B loss: 123.669434]\n",
      "[Epoch 0/100] [Batch 474/1463] [D_A loss: 0.040175] [D_B loss: -0.066047] [G_A loss: 12.463882, G_B loss: 84.935280]\n",
      "[Epoch 0/100] [Batch 475/1463] [D_A loss: -0.376464] [D_B loss: 0.105117] [G_A loss: 14.996275, G_B loss: 55.872612]\n",
      "[Epoch 0/100] [Batch 476/1463] [D_A loss: 1.597173] [D_B loss: -0.157637] [G_A loss: 15.147821, G_B loss: 85.750603]\n",
      "[Epoch 0/100] [Batch 477/1463] [D_A loss: -0.218741] [D_B loss: -0.042675] [G_A loss: 17.300903, G_B loss: 97.743134]\n",
      "[Epoch 0/100] [Batch 478/1463] [D_A loss: -0.160682] [D_B loss: -0.003340] [G_A loss: 16.363363, G_B loss: 77.118843]\n",
      "[Epoch 0/100] [Batch 479/1463] [D_A loss: -1.471829] [D_B loss: 0.101815] [G_A loss: 13.255581, G_B loss: -6.460035]\n",
      "[Epoch 0/100] [Batch 480/1463] [D_A loss: -0.005814] [D_B loss: -0.012548] [G_A loss: 12.628453, G_B loss: 83.102951]\n",
      "[Epoch 0/100] [Batch 481/1463] [D_A loss: 1.512881] [D_B loss: -0.044454] [G_A loss: 13.186615, G_B loss: 32.295254]\n",
      "[Epoch 0/100] [Batch 482/1463] [D_A loss: -2.062972] [D_B loss: -0.087423] [G_A loss: 16.471462, G_B loss: 187.272705]\n",
      "[Epoch 0/100] [Batch 483/1463] [D_A loss: 2.682360] [D_B loss: 0.133267] [G_A loss: 15.137139, G_B loss: 111.060410]\n",
      "[Epoch 0/100] [Batch 484/1463] [D_A loss: -2.172847] [D_B loss: 0.074595] [G_A loss: 11.072417, G_B loss: -33.155121]\n",
      "[Epoch 0/100] [Batch 485/1463] [D_A loss: 0.848250] [D_B loss: 0.225977] [G_A loss: 12.845872, G_B loss: -16.692543]\n",
      "[Epoch 0/100] [Batch 486/1463] [D_A loss: 1.541181] [D_B loss: 0.042612] [G_A loss: 12.439441, G_B loss: 142.062210]\n",
      "[Epoch 0/100] [Batch 487/1463] [D_A loss: -3.336786] [D_B loss: -0.004576] [G_A loss: 11.957068, G_B loss: 234.677338]\n",
      "[Epoch 0/100] [Batch 488/1463] [D_A loss: -0.481246] [D_B loss: -0.024819] [G_A loss: 10.811644, G_B loss: 193.506195]\n",
      "[Epoch 0/100] [Batch 489/1463] [D_A loss: 2.858470] [D_B loss: 0.018237] [G_A loss: 12.245438, G_B loss: 94.202652]\n",
      "[Epoch 0/100] [Batch 490/1463] [D_A loss: -0.129083] [D_B loss: 0.087052] [G_A loss: 13.832508, G_B loss: 18.182787]\n",
      "[Epoch 0/100] [Batch 491/1463] [D_A loss: -4.996682] [D_B loss: 0.103636] [G_A loss: 12.336411, G_B loss: -63.435909]\n",
      "[Epoch 0/100] [Batch 492/1463] [D_A loss: 2.347599] [D_B loss: -0.113976] [G_A loss: 13.363670, G_B loss: -4.210124]\n",
      "[Epoch 0/100] [Batch 493/1463] [D_A loss: -0.046950] [D_B loss: -0.121849] [G_A loss: 14.021584, G_B loss: 110.997810]\n",
      "[Epoch 0/100] [Batch 494/1463] [D_A loss: -4.069084] [D_B loss: -0.128852] [G_A loss: 14.924550, G_B loss: 167.418335]\n",
      "[Epoch 0/100] [Batch 495/1463] [D_A loss: 2.518072] [D_B loss: 0.103049] [G_A loss: 18.473469, G_B loss: 174.742706]\n",
      "[Epoch 0/100] [Batch 496/1463] [D_A loss: 2.283357] [D_B loss: -0.088117] [G_A loss: 23.087910, G_B loss: 113.912132]\n",
      "[Epoch 0/100] [Batch 497/1463] [D_A loss: -0.345368] [D_B loss: -0.177203] [G_A loss: 25.973707, G_B loss: 47.183529]\n",
      "[Epoch 0/100] [Batch 498/1463] [D_A loss: 0.191099] [D_B loss: 0.039343] [G_A loss: 22.984253, G_B loss: 4.743131]\n",
      "[Epoch 0/100] [Batch 499/1463] [D_A loss: 1.207946] [D_B loss: -0.369449] [G_A loss: 22.013758, G_B loss: -19.171270]\n",
      "[Epoch 0/100] [Batch 500/1463] [D_A loss: 0.655738] [D_B loss: -0.644846] [G_A loss: 20.058342, G_B loss: 116.265564]\n",
      "[Epoch 0/100] [Batch 501/1463] [D_A loss: -0.600048] [D_B loss: 0.832658] [G_A loss: 18.409340, G_B loss: 140.139801]\n",
      "[Epoch 0/100] [Batch 502/1463] [D_A loss: 1.035317] [D_B loss: -0.046529] [G_A loss: 19.773039, G_B loss: 94.929993]\n",
      "[Epoch 0/100] [Batch 503/1463] [D_A loss: 0.465817] [D_B loss: -0.088282] [G_A loss: 21.067791, G_B loss: 21.530800]\n",
      "[Epoch 0/100] [Batch 504/1463] [D_A loss: 0.484677] [D_B loss: 0.040715] [G_A loss: 20.974455, G_B loss: 17.495668]\n",
      "[Epoch 0/100] [Batch 505/1463] [D_A loss: -0.290274] [D_B loss: 0.001384] [G_A loss: 20.162880, G_B loss: 30.246098]\n",
      "[Epoch 0/100] [Batch 506/1463] [D_A loss: 1.381586] [D_B loss: -0.195507] [G_A loss: 19.099201, G_B loss: 85.744370]\n",
      "[Epoch 0/100] [Batch 507/1463] [D_A loss: -0.207466] [D_B loss: -0.214306] [G_A loss: 18.873089, G_B loss: 21.592955]\n",
      "[Epoch 0/100] [Batch 508/1463] [D_A loss: 0.812156] [D_B loss: -0.138931] [G_A loss: 19.182615, G_B loss: 5.036523]\n",
      "[Epoch 0/100] [Batch 509/1463] [D_A loss: 0.034848] [D_B loss: 0.284065] [G_A loss: 14.874150, G_B loss: 110.155396]\n",
      "[Epoch 0/100] [Batch 510/1463] [D_A loss: 1.019558] [D_B loss: 0.219678] [G_A loss: 12.191653, G_B loss: 112.206573]\n",
      "[Epoch 0/100] [Batch 511/1463] [D_A loss: -0.454256] [D_B loss: -0.213294] [G_A loss: 11.886612, G_B loss: 27.367336]\n",
      "[Epoch 0/100] [Batch 512/1463] [D_A loss: 0.346584] [D_B loss: 0.142198] [G_A loss: 12.598932, G_B loss: 50.538517]\n",
      "[Epoch 0/100] [Batch 513/1463] [D_A loss: -0.862102] [D_B loss: -0.007829] [G_A loss: 16.901703, G_B loss: 173.918823]\n",
      "[Epoch 0/100] [Batch 514/1463] [D_A loss: 2.420694] [D_B loss: -0.025746] [G_A loss: 19.154713, G_B loss: 122.218826]\n",
      "[Epoch 0/100] [Batch 515/1463] [D_A loss: -0.025640] [D_B loss: -0.310379] [G_A loss: 21.951805, G_B loss: 88.345314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 516/1463] [D_A loss: -2.575162] [D_B loss: 0.146241] [G_A loss: 19.583708, G_B loss: -30.820450]\n",
      "[Epoch 0/100] [Batch 517/1463] [D_A loss: 0.012522] [D_B loss: 0.003594] [G_A loss: 18.958775, G_B loss: -31.477276]\n",
      "[Epoch 0/100] [Batch 518/1463] [D_A loss: 1.432787] [D_B loss: -0.314750] [G_A loss: 18.968697, G_B loss: 110.232407]\n",
      "[Epoch 0/100] [Batch 519/1463] [D_A loss: 0.894515] [D_B loss: 0.064844] [G_A loss: 14.516290, G_B loss: 92.924713]\n",
      "[Epoch 0/100] [Batch 520/1463] [D_A loss: -0.630604] [D_B loss: 0.161998] [G_A loss: 13.492724, G_B loss: 106.794937]\n",
      "[Epoch 0/100] [Batch 521/1463] [D_A loss: -0.586298] [D_B loss: 0.039946] [G_A loss: 14.758097, G_B loss: 53.585590]\n",
      "[Epoch 0/100] [Batch 522/1463] [D_A loss: -0.481173] [D_B loss: 0.015903] [G_A loss: 15.713711, G_B loss: 83.122734]\n",
      "[Epoch 0/100] [Batch 523/1463] [D_A loss: -0.321624] [D_B loss: -0.286005] [G_A loss: 16.143497, G_B loss: 165.616180]\n",
      "[Epoch 0/100] [Batch 524/1463] [D_A loss: 0.338962] [D_B loss: 0.312922] [G_A loss: 13.739488, G_B loss: 47.094337]\n",
      "[Epoch 0/100] [Batch 525/1463] [D_A loss: -1.048831] [D_B loss: 0.079224] [G_A loss: 10.938084, G_B loss: -26.892841]\n",
      "[Epoch 0/100] [Batch 526/1463] [D_A loss: 1.489731] [D_B loss: 0.119802] [G_A loss: 10.695341, G_B loss: 115.952263]\n",
      "[Epoch 0/100] [Batch 527/1463] [D_A loss: 0.241195] [D_B loss: -0.005830] [G_A loss: 8.645560, G_B loss: 132.390366]\n",
      "[Epoch 0/100] [Batch 528/1463] [D_A loss: 0.437941] [D_B loss: 0.072150] [G_A loss: 11.083463, G_B loss: 142.930817]\n",
      "[Epoch 0/100] [Batch 529/1463] [D_A loss: 0.196721] [D_B loss: 0.312819] [G_A loss: 12.152400, G_B loss: 102.832077]\n",
      "[Epoch 0/100] [Batch 530/1463] [D_A loss: -0.285545] [D_B loss: -0.018519] [G_A loss: 11.902093, G_B loss: 21.793468]\n",
      "[Epoch 0/100] [Batch 531/1463] [D_A loss: -1.500071] [D_B loss: -0.088520] [G_A loss: 12.202344, G_B loss: 120.935860]\n",
      "[Epoch 0/100] [Batch 532/1463] [D_A loss: 3.267480] [D_B loss: 0.234335] [G_A loss: 12.934984, G_B loss: 89.187424]\n",
      "[Epoch 0/100] [Batch 533/1463] [D_A loss: 0.099771] [D_B loss: -0.005668] [G_A loss: 13.694746, G_B loss: 39.281395]\n",
      "[Epoch 0/100] [Batch 534/1463] [D_A loss: -1.675301] [D_B loss: 0.029632] [G_A loss: 13.863983, G_B loss: 57.677608]\n",
      "[Epoch 0/100] [Batch 535/1463] [D_A loss: -2.525677] [D_B loss: 0.044067] [G_A loss: 12.481977, G_B loss: 73.640755]\n",
      "[Epoch 0/100] [Batch 536/1463] [D_A loss: 1.655762] [D_B loss: 0.050003] [G_A loss: 13.081438, G_B loss: 101.622673]\n",
      "[Epoch 0/100] [Batch 537/1463] [D_A loss: -2.532213] [D_B loss: -0.053703] [G_A loss: 15.469044, G_B loss: 140.856201]\n",
      "[Epoch 0/100] [Batch 538/1463] [D_A loss: -2.273624] [D_B loss: -0.046082] [G_A loss: 14.914054, G_B loss: 113.164505]\n",
      "[Epoch 0/100] [Batch 539/1463] [D_A loss: -3.069954] [D_B loss: -0.169979] [G_A loss: 16.072269, G_B loss: 52.252033]\n",
      "[Epoch 0/100] [Batch 540/1463] [D_A loss: 0.605750] [D_B loss: -0.002756] [G_A loss: 17.317493, G_B loss: 97.154793]\n",
      "[Epoch 0/100] [Batch 541/1463] [D_A loss: 4.008185] [D_B loss: -0.010021] [G_A loss: 15.468351, G_B loss: -27.892937]\n",
      "[Epoch 0/100] [Batch 542/1463] [D_A loss: -3.887588] [D_B loss: -0.078078] [G_A loss: 17.779297, G_B loss: 115.727821]\n",
      "[Epoch 0/100] [Batch 543/1463] [D_A loss: -3.537324] [D_B loss: -0.260124] [G_A loss: 19.693176, G_B loss: 183.458221]\n",
      "[Epoch 0/100] [Batch 544/1463] [D_A loss: 1.607584] [D_B loss: -0.514601] [G_A loss: 26.926374, G_B loss: 208.834518]\n",
      "[Epoch 0/100] [Batch 545/1463] [D_A loss: -1.819172] [D_B loss: 0.774489] [G_A loss: 24.479856, G_B loss: 168.376312]\n",
      "[Epoch 0/100] [Batch 546/1463] [D_A loss: -5.430312] [D_B loss: -0.048913] [G_A loss: 23.842474, G_B loss: 131.908737]\n",
      "[Epoch 0/100] [Batch 547/1463] [D_A loss: -1.930974] [D_B loss: -0.443300] [G_A loss: 26.272079, G_B loss: 97.538750]\n",
      "[Epoch 0/100] [Batch 548/1463] [D_A loss: -2.148095] [D_B loss: -0.114189] [G_A loss: 25.005548, G_B loss: 45.769466]\n",
      "[Epoch 0/100] [Batch 549/1463] [D_A loss: 0.113656] [D_B loss: 0.221325] [G_A loss: 18.815550, G_B loss: -58.836197]\n",
      "[Epoch 0/100] [Batch 550/1463] [D_A loss: -7.819216] [D_B loss: 0.006258] [G_A loss: 17.857100, G_B loss: -118.414444]\n",
      "[Epoch 0/100] [Batch 551/1463] [D_A loss: -2.733791] [D_B loss: 0.107271] [G_A loss: 18.730467, G_B loss: -127.114250]\n",
      "[Epoch 0/100] [Batch 552/1463] [D_A loss: 4.557053] [D_B loss: -0.166078] [G_A loss: 24.546900, G_B loss: -33.626102]\n",
      "[Epoch 0/100] [Batch 553/1463] [D_A loss: 5.944392] [D_B loss: -0.203231] [G_A loss: 25.321880, G_B loss: 112.947784]\n",
      "[Epoch 0/100] [Batch 554/1463] [D_A loss: -1.032607] [D_B loss: -0.187084] [G_A loss: 28.294769, G_B loss: 231.444839]\n",
      "[Epoch 0/100] [Batch 555/1463] [D_A loss: -11.954578] [D_B loss: -0.079151] [G_A loss: 28.878134, G_B loss: 331.268311]\n",
      "[Epoch 0/100] [Batch 556/1463] [D_A loss: 7.543355] [D_B loss: 0.307530] [G_A loss: 21.023203, G_B loss: 278.787079]\n",
      "[Epoch 0/100] [Batch 557/1463] [D_A loss: -6.235577] [D_B loss: -0.093315] [G_A loss: 16.786289, G_B loss: 306.043884]\n",
      "[Epoch 0/100] [Batch 558/1463] [D_A loss: -5.870405] [D_B loss: -0.423586] [G_A loss: 18.407158, G_B loss: 290.505310]\n",
      "[Epoch 0/100] [Batch 559/1463] [D_A loss: -19.266176] [D_B loss: 0.477181] [G_A loss: 17.785887, G_B loss: 252.606903]\n",
      "[Epoch 0/100] [Batch 560/1463] [D_A loss: -28.797134] [D_B loss: 0.057099] [G_A loss: 20.988832, G_B loss: 190.950943]\n",
      "[Epoch 0/100] [Batch 561/1463] [D_A loss: -7.360093] [D_B loss: -0.219945] [G_A loss: 25.543659, G_B loss: 71.963654]\n",
      "[Epoch 0/100] [Batch 562/1463] [D_A loss: 15.759274] [D_B loss: 0.029879] [G_A loss: 28.710030, G_B loss: 26.019760]\n",
      "[Epoch 0/100] [Batch 563/1463] [D_A loss: 5.593984] [D_B loss: -0.286291] [G_A loss: 29.279654, G_B loss: -31.303915]\n",
      "[Epoch 0/100] [Batch 564/1463] [D_A loss: 14.439286] [D_B loss: -0.073349] [G_A loss: 23.880434, G_B loss: -18.377842]\n",
      "[Epoch 0/100] [Batch 565/1463] [D_A loss: 4.429594] [D_B loss: -0.195186] [G_A loss: 20.659695, G_B loss: 0.006353]\n",
      "[Epoch 0/100] [Batch 566/1463] [D_A loss: 0.767616] [D_B loss: -0.112504] [G_A loss: 19.145010, G_B loss: 28.435751]\n",
      "[Epoch 0/100] [Batch 567/1463] [D_A loss: -2.403873] [D_B loss: 0.174912] [G_A loss: 20.243990, G_B loss: 79.158768]\n",
      "[Epoch 0/100] [Batch 568/1463] [D_A loss: -3.461598] [D_B loss: 0.097814] [G_A loss: 21.742846, G_B loss: 86.554092]\n",
      "[Epoch 0/100] [Batch 569/1463] [D_A loss: -6.643184] [D_B loss: -0.007223] [G_A loss: 26.578131, G_B loss: 163.790512]\n",
      "[Epoch 0/100] [Batch 570/1463] [D_A loss: -2.181712] [D_B loss: 0.201540] [G_A loss: 28.586405, G_B loss: 134.715317]\n",
      "[Epoch 0/100] [Batch 571/1463] [D_A loss: -4.177874] [D_B loss: 0.051423] [G_A loss: 28.891695, G_B loss: 93.553696]\n",
      "[Epoch 0/100] [Batch 572/1463] [D_A loss: -3.506691] [D_B loss: 0.004869] [G_A loss: 27.062056, G_B loss: 9.521710]\n",
      "[Epoch 0/100] [Batch 573/1463] [D_A loss: -4.355032] [D_B loss: -0.277177] [G_A loss: 22.340294, G_B loss: 2.560099]\n",
      "[Epoch 0/100] [Batch 574/1463] [D_A loss: -0.688064] [D_B loss: -0.310698] [G_A loss: 19.649372, G_B loss: 22.066423]\n",
      "[Epoch 0/100] [Batch 575/1463] [D_A loss: -0.504517] [D_B loss: 0.035444] [G_A loss: 19.954168, G_B loss: 51.619080]\n",
      "[Epoch 0/100] [Batch 576/1463] [D_A loss: -0.048581] [D_B loss: 0.175546] [G_A loss: 21.034637, G_B loss: 140.336746]\n",
      "[Epoch 0/100] [Batch 577/1463] [D_A loss: -0.753512] [D_B loss: -0.165554] [G_A loss: 24.747967, G_B loss: 160.515366]\n",
      "[Epoch 0/100] [Batch 578/1463] [D_A loss: -4.578062] [D_B loss: -0.238145] [G_A loss: 27.762804, G_B loss: 178.291138]\n",
      "[Epoch 0/100] [Batch 579/1463] [D_A loss: 1.226111] [D_B loss: -0.190939] [G_A loss: 31.419453, G_B loss: 218.985016]\n",
      "[Epoch 0/100] [Batch 580/1463] [D_A loss: -1.082602] [D_B loss: -0.372314] [G_A loss: 27.430975, G_B loss: 137.815109]\n",
      "[Epoch 0/100] [Batch 581/1463] [D_A loss: -0.175082] [D_B loss: 0.750380] [G_A loss: 19.826740, G_B loss: 118.458145]\n",
      "[Epoch 0/100] [Batch 582/1463] [D_A loss: -1.937193] [D_B loss: -0.279918] [G_A loss: 19.759962, G_B loss: 137.717010]\n",
      "[Epoch 0/100] [Batch 583/1463] [D_A loss: -3.532158] [D_B loss: -0.898836] [G_A loss: 22.712542, G_B loss: 134.014053]\n",
      "[Epoch 0/100] [Batch 584/1463] [D_A loss: -3.081305] [D_B loss: -0.345069] [G_A loss: 22.900368, G_B loss: 93.057510]\n",
      "[Epoch 0/100] [Batch 585/1463] [D_A loss: -1.171589] [D_B loss: 0.013800] [G_A loss: 24.198793, G_B loss: 25.989201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 586/1463] [D_A loss: -0.281707] [D_B loss: 0.152818] [G_A loss: 26.331938, G_B loss: 58.083889]\n",
      "[Epoch 0/100] [Batch 587/1463] [D_A loss: 1.045324] [D_B loss: -0.220862] [G_A loss: 29.854132, G_B loss: 187.083023]\n",
      "[Epoch 0/100] [Batch 588/1463] [D_A loss: 1.825454] [D_B loss: -0.316573] [G_A loss: 32.931007, G_B loss: 221.122284]\n",
      "[Epoch 0/100] [Batch 589/1463] [D_A loss: 0.012722] [D_B loss: -0.055780] [G_A loss: 35.517136, G_B loss: 211.335663]\n",
      "[Epoch 0/100] [Batch 590/1463] [D_A loss: -1.083031] [D_B loss: 0.127826] [G_A loss: 36.017410, G_B loss: 69.858803]\n",
      "[Epoch 0/100] [Batch 591/1463] [D_A loss: 0.546446] [D_B loss: 0.036183] [G_A loss: 33.254395, G_B loss: 17.580729]\n",
      "[Epoch 0/100] [Batch 592/1463] [D_A loss: -0.821432] [D_B loss: -0.434144] [G_A loss: 35.066277, G_B loss: 102.263824]\n",
      "[Epoch 0/100] [Batch 593/1463] [D_A loss: -1.607527] [D_B loss: 0.337021] [G_A loss: 37.161888, G_B loss: 120.783379]\n",
      "[Epoch 0/100] [Batch 594/1463] [D_A loss: -1.465072] [D_B loss: -0.261787] [G_A loss: 36.712746, G_B loss: 70.934578]\n",
      "[Epoch 0/100] [Batch 595/1463] [D_A loss: 0.242446] [D_B loss: -0.043027] [G_A loss: 36.436378, G_B loss: 121.495926]\n",
      "[Epoch 0/100] [Batch 596/1463] [D_A loss: -2.922023] [D_B loss: -0.215073] [G_A loss: 40.398281, G_B loss: 193.927124]\n",
      "[Epoch 0/100] [Batch 597/1463] [D_A loss: 0.442832] [D_B loss: -0.413137] [G_A loss: 39.697212, G_B loss: 177.042252]\n",
      "[Epoch 0/100] [Batch 598/1463] [D_A loss: -0.406258] [D_B loss: -0.296948] [G_A loss: 40.477684, G_B loss: 127.268028]\n",
      "[Epoch 0/100] [Batch 599/1463] [D_A loss: -1.264509] [D_B loss: -0.374472] [G_A loss: 39.433350, G_B loss: 22.367451]\n",
      "[Epoch 0/100] [Batch 600/1463] [D_A loss: -1.616885] [D_B loss: -0.412125] [G_A loss: 34.933311, G_B loss: 3.013940]\n",
      "[Epoch 0/100] [Batch 601/1463] [D_A loss: 0.988425] [D_B loss: 0.117241] [G_A loss: 33.133141, G_B loss: 55.287941]\n",
      "[Epoch 0/100] [Batch 602/1463] [D_A loss: 0.388094] [D_B loss: 0.289035] [G_A loss: 33.458500, G_B loss: 159.645493]\n",
      "[Epoch 0/100] [Batch 603/1463] [D_A loss: -1.371621] [D_B loss: -0.175667] [G_A loss: 36.437538, G_B loss: 174.704315]\n",
      "[Epoch 0/100] [Batch 604/1463] [D_A loss: 0.391332] [D_B loss: -0.877235] [G_A loss: 41.265877, G_B loss: 207.329803]\n",
      "[Epoch 0/100] [Batch 605/1463] [D_A loss: -1.818155] [D_B loss: -0.956841] [G_A loss: 44.883835, G_B loss: 114.653084]\n",
      "[Epoch 0/100] [Batch 606/1463] [D_A loss: -2.764280] [D_B loss: -0.493832] [G_A loss: 37.504780, G_B loss: -7.356498]\n",
      "[Epoch 0/100] [Batch 607/1463] [D_A loss: 0.397568] [D_B loss: 0.164077] [G_A loss: 37.121609, G_B loss: 42.422630]\n",
      "[Epoch 0/100] [Batch 608/1463] [D_A loss: -2.361939] [D_B loss: -0.098537] [G_A loss: 39.130508, G_B loss: 196.879379]\n",
      "[Epoch 0/100] [Batch 609/1463] [D_A loss: 0.879978] [D_B loss: -0.414532] [G_A loss: 41.466522, G_B loss: 119.019493]\n",
      "[Epoch 0/100] [Batch 610/1463] [D_A loss: 5.516540] [D_B loss: 0.632693] [G_A loss: 46.843132, G_B loss: 35.316906]\n",
      "[Epoch 0/100] [Batch 611/1463] [D_A loss: 1.692685] [D_B loss: 0.193315] [G_A loss: 50.841194, G_B loss: 28.001337]\n",
      "[Epoch 0/100] [Batch 612/1463] [D_A loss: -1.200888] [D_B loss: 0.121680] [G_A loss: 50.833290, G_B loss: 156.852325]\n",
      "[Epoch 0/100] [Batch 613/1463] [D_A loss: 0.083853] [D_B loss: -0.119966] [G_A loss: 49.368614, G_B loss: 159.024933]\n",
      "[Epoch 0/100] [Batch 614/1463] [D_A loss: -0.839262] [D_B loss: -0.158644] [G_A loss: 44.020107, G_B loss: 54.326565]\n",
      "[Epoch 0/100] [Batch 615/1463] [D_A loss: -0.153325] [D_B loss: -0.104837] [G_A loss: 44.888897, G_B loss: 34.052071]\n",
      "[Epoch 0/100] [Batch 616/1463] [D_A loss: 0.160063] [D_B loss: -0.125167] [G_A loss: 47.714409, G_B loss: 23.482693]\n",
      "[Epoch 0/100] [Batch 617/1463] [D_A loss: 2.143777] [D_B loss: -0.457624] [G_A loss: 58.709824, G_B loss: 114.972778]\n",
      "[Epoch 0/100] [Batch 618/1463] [D_A loss: -1.669148] [D_B loss: -0.338581] [G_A loss: 55.167503, G_B loss: 173.279877]\n",
      "[Epoch 0/100] [Batch 619/1463] [D_A loss: 1.962198] [D_B loss: 0.541238] [G_A loss: 43.352379, G_B loss: 67.087387]\n",
      "[Epoch 0/100] [Batch 620/1463] [D_A loss: -2.738557] [D_B loss: -0.073113] [G_A loss: 42.980896, G_B loss: -10.564892]\n",
      "[Epoch 0/100] [Batch 621/1463] [D_A loss: -0.660763] [D_B loss: -0.048831] [G_A loss: 42.339390, G_B loss: 87.044426]\n",
      "[Epoch 0/100] [Batch 622/1463] [D_A loss: -0.545307] [D_B loss: -0.014770] [G_A loss: 46.018101, G_B loss: 132.572052]\n",
      "[Epoch 0/100] [Batch 623/1463] [D_A loss: -0.326452] [D_B loss: -0.434466] [G_A loss: 49.698994, G_B loss: 110.498734]\n",
      "[Epoch 0/100] [Batch 624/1463] [D_A loss: -0.239367] [D_B loss: -0.382298] [G_A loss: 52.342407, G_B loss: 46.205074]\n",
      "[Epoch 0/100] [Batch 625/1463] [D_A loss: 0.279768] [D_B loss: -0.196801] [G_A loss: 49.300274, G_B loss: 26.883158]\n",
      "[Epoch 0/100] [Batch 626/1463] [D_A loss: 0.230135] [D_B loss: 0.256800] [G_A loss: 43.278412, G_B loss: 83.526512]\n",
      "[Epoch 0/100] [Batch 627/1463] [D_A loss: -0.522911] [D_B loss: 0.071987] [G_A loss: 44.100842, G_B loss: 161.715073]\n",
      "[Epoch 0/100] [Batch 628/1463] [D_A loss: 1.571354] [D_B loss: 0.325538] [G_A loss: 40.969746, G_B loss: 142.041733]\n",
      "[Epoch 0/100] [Batch 629/1463] [D_A loss: 0.455041] [D_B loss: -0.216715] [G_A loss: 42.528500, G_B loss: 112.745064]\n",
      "[Epoch 0/100] [Batch 630/1463] [D_A loss: -0.349570] [D_B loss: -0.484695] [G_A loss: 39.611824, G_B loss: 81.658592]\n",
      "[Epoch 0/100] [Batch 631/1463] [D_A loss: 0.334196] [D_B loss: 0.525614] [G_A loss: 42.088280, G_B loss: -35.376366]\n",
      "[Epoch 0/100] [Batch 632/1463] [D_A loss: 4.095667] [D_B loss: -0.055145] [G_A loss: 45.618053, G_B loss: 60.590233]\n",
      "[Epoch 0/100] [Batch 633/1463] [D_A loss: -0.975093] [D_B loss: 0.241421] [G_A loss: 47.894638, G_B loss: 164.695862]\n",
      "[Epoch 0/100] [Batch 634/1463] [D_A loss: -1.626122] [D_B loss: -0.090565] [G_A loss: 44.612812, G_B loss: 188.885269]\n",
      "[Epoch 0/100] [Batch 635/1463] [D_A loss: -0.821673] [D_B loss: 0.366012] [G_A loss: 39.728428, G_B loss: 110.477829]\n",
      "[Epoch 0/100] [Batch 636/1463] [D_A loss: 2.058834] [D_B loss: -0.030099] [G_A loss: 36.304573, G_B loss: 54.368504]\n",
      "[Epoch 0/100] [Batch 637/1463] [D_A loss: -0.823599] [D_B loss: -0.328195] [G_A loss: 32.614269, G_B loss: 40.581303]\n",
      "[Epoch 0/100] [Batch 638/1463] [D_A loss: 0.601829] [D_B loss: 0.002692] [G_A loss: 36.471626, G_B loss: -2.837231]\n",
      "[Epoch 0/100] [Batch 639/1463] [D_A loss: -0.600712] [D_B loss: -0.097563] [G_A loss: 32.901985, G_B loss: 107.450378]\n",
      "[Epoch 0/100] [Batch 640/1463] [D_A loss: -0.400929] [D_B loss: -0.303880] [G_A loss: 38.685612, G_B loss: 110.812630]\n",
      "[Epoch 0/100] [Batch 641/1463] [D_A loss: 0.236283] [D_B loss: 0.179145] [G_A loss: 37.974705, G_B loss: 50.059128]\n",
      "[Epoch 0/100] [Batch 642/1463] [D_A loss: 0.948311] [D_B loss: 0.063525] [G_A loss: 33.729374, G_B loss: 68.172379]\n",
      "[Epoch 0/100] [Batch 643/1463] [D_A loss: 0.485641] [D_B loss: 0.031210] [G_A loss: 29.375565, G_B loss: 88.928055]\n",
      "[Epoch 0/100] [Batch 644/1463] [D_A loss: -0.896370] [D_B loss: -0.112702] [G_A loss: 28.886452, G_B loss: 117.671768]\n",
      "[Epoch 0/100] [Batch 645/1463] [D_A loss: -2.848592] [D_B loss: -0.086043] [G_A loss: 31.898502, G_B loss: 192.385635]\n",
      "[Epoch 0/100] [Batch 646/1463] [D_A loss: -0.154291] [D_B loss: -0.048779] [G_A loss: 35.102692, G_B loss: 117.664749]\n",
      "[Epoch 0/100] [Batch 647/1463] [D_A loss: 1.219154] [D_B loss: -0.371581] [G_A loss: 43.305626, G_B loss: 64.912201]\n",
      "[Epoch 0/100] [Batch 648/1463] [D_A loss: 0.855344] [D_B loss: 0.014624] [G_A loss: 41.023323, G_B loss: 21.237877]\n",
      "[Epoch 0/100] [Batch 649/1463] [D_A loss: -0.009062] [D_B loss: 0.073211] [G_A loss: 37.827507, G_B loss: 50.724171]\n",
      "[Epoch 0/100] [Batch 650/1463] [D_A loss: -2.311239] [D_B loss: 0.167719] [G_A loss: 34.347626, G_B loss: 195.209381]\n",
      "[Epoch 0/100] [Batch 651/1463] [D_A loss: -3.446155] [D_B loss: -0.059895] [G_A loss: 33.948917, G_B loss: 199.392487]\n",
      "[Epoch 0/100] [Batch 652/1463] [D_A loss: 3.928882] [D_B loss: 0.193704] [G_A loss: 35.886951, G_B loss: 114.320946]\n",
      "[Epoch 0/100] [Batch 653/1463] [D_A loss: 0.598066] [D_B loss: -0.093744] [G_A loss: 38.428185, G_B loss: 71.267746]\n",
      "[Epoch 0/100] [Batch 654/1463] [D_A loss: -3.263028] [D_B loss: -0.570572] [G_A loss: 45.653603, G_B loss: -0.812872]\n",
      "[Epoch 0/100] [Batch 655/1463] [D_A loss: -1.380041] [D_B loss: 0.193972] [G_A loss: 40.244553, G_B loss: 22.926935]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 656/1463] [D_A loss: -3.406855] [D_B loss: 0.132232] [G_A loss: 32.854378, G_B loss: -8.060356]\n",
      "[Epoch 0/100] [Batch 657/1463] [D_A loss: 0.402509] [D_B loss: -0.435495] [G_A loss: 29.746902, G_B loss: 24.061064]\n",
      "[Epoch 0/100] [Batch 658/1463] [D_A loss: -0.226442] [D_B loss: -0.079502] [G_A loss: 28.297287, G_B loss: 131.506378]\n",
      "[Epoch 0/100] [Batch 659/1463] [D_A loss: -2.982185] [D_B loss: -0.037243] [G_A loss: 28.868032, G_B loss: 227.379044]\n",
      "[Epoch 0/100] [Batch 660/1463] [D_A loss: -2.922874] [D_B loss: -0.108698] [G_A loss: 35.913544, G_B loss: 243.680573]\n",
      "[Epoch 0/100] [Batch 661/1463] [D_A loss: 1.154917] [D_B loss: -0.309115] [G_A loss: 40.707642, G_B loss: 184.676315]\n",
      "[Epoch 0/100] [Batch 662/1463] [D_A loss: 7.708685] [D_B loss: 0.010305] [G_A loss: 41.109730, G_B loss: 83.033501]\n",
      "[Epoch 0/100] [Batch 663/1463] [D_A loss: 0.500891] [D_B loss: -0.252395] [G_A loss: 46.929070, G_B loss: 60.631561]\n",
      "[Epoch 0/100] [Batch 664/1463] [D_A loss: -0.543774] [D_B loss: 0.055349] [G_A loss: 42.819534, G_B loss: 127.291451]\n",
      "[Epoch 0/100] [Batch 665/1463] [D_A loss: -1.194829] [D_B loss: -0.029746] [G_A loss: 42.224586, G_B loss: 135.734283]\n",
      "[Epoch 0/100] [Batch 666/1463] [D_A loss: -2.774608] [D_B loss: -0.255318] [G_A loss: 40.498466, G_B loss: 96.696274]\n",
      "[Epoch 0/100] [Batch 667/1463] [D_A loss: -0.155581] [D_B loss: -0.101062] [G_A loss: 37.875641, G_B loss: 72.483429]\n",
      "[Epoch 0/100] [Batch 668/1463] [D_A loss: -0.948820] [D_B loss: -0.551816] [G_A loss: 39.422073, G_B loss: 62.417847]\n",
      "[Epoch 0/100] [Batch 669/1463] [D_A loss: -1.136937] [D_B loss: 0.004695] [G_A loss: 40.926968, G_B loss: 112.198158]\n",
      "[Epoch 0/100] [Batch 670/1463] [D_A loss: -1.067336] [D_B loss: 0.298037] [G_A loss: 42.659988, G_B loss: 140.256775]\n",
      "[Epoch 0/100] [Batch 671/1463] [D_A loss: -0.728956] [D_B loss: -0.151060] [G_A loss: 45.533939, G_B loss: 117.943100]\n",
      "[Epoch 0/100] [Batch 672/1463] [D_A loss: -0.944759] [D_B loss: -0.365820] [G_A loss: 45.994293, G_B loss: 163.103470]\n",
      "[Epoch 0/100] [Batch 673/1463] [D_A loss: 0.254050] [D_B loss: -0.385056] [G_A loss: 41.540253, G_B loss: 46.133644]\n",
      "[Epoch 0/100] [Batch 674/1463] [D_A loss: 0.039402] [D_B loss: -0.182655] [G_A loss: 37.330376, G_B loss: 119.720184]\n",
      "[Epoch 0/100] [Batch 675/1463] [D_A loss: 0.673846] [D_B loss: 0.399728] [G_A loss: 36.192303, G_B loss: 111.684433]\n",
      "[Epoch 0/100] [Batch 676/1463] [D_A loss: 0.054928] [D_B loss: 0.348976] [G_A loss: 35.576183, G_B loss: 141.102753]\n",
      "[Epoch 0/100] [Batch 677/1463] [D_A loss: 0.502969] [D_B loss: -0.023163] [G_A loss: 38.356480, G_B loss: 118.352692]\n",
      "[Epoch 0/100] [Batch 678/1463] [D_A loss: -0.638895] [D_B loss: -0.191932] [G_A loss: 41.335064, G_B loss: 163.584122]\n",
      "[Epoch 0/100] [Batch 679/1463] [D_A loss: 0.203796] [D_B loss: -0.331967] [G_A loss: 42.698650, G_B loss: 102.122086]\n",
      "[Epoch 0/100] [Batch 680/1463] [D_A loss: -0.339583] [D_B loss: 0.532530] [G_A loss: 39.935009, G_B loss: 146.131592]\n",
      "[Epoch 0/100] [Batch 681/1463] [D_A loss: 0.131140] [D_B loss: -0.058134] [G_A loss: 34.366920, G_B loss: 56.521046]\n"
     ]
    }
   ],
   "source": [
    "sample_interval = 25\n",
    "checkpoint_interval = 100\n",
    "file_prefix = proj_root + 'saved_models/dual_wgans2/'\n",
    "\n",
    "e_tracker = EpochTracker(file_prefix + 'epoch.txt')\n",
    "\n",
    "if(e_tracker.file_exists):\n",
    "    gen_a.load_state_dict(torch.load(file_prefix + 'generator_a.pth'))\n",
    "    dis_a.load_state_dict(torch.load(file_prefix + 'discriminator_a.pth'))\n",
    "    gen_b.load_state_dict(torch.load(file_prefix + 'generator_b.pth'))\n",
    "    dis_b.load_state_dict(torch.load(file_prefix + 'discriminator_b.pth'))\n",
    "\n",
    "    \n",
    "for epoch in range(e_tracker.epoch, num_epochs):\n",
    "    for i in range(num_images // batch_size):\n",
    "        if epoch == e_tracker.epoch and i <= e_tracker.iter:\n",
    "            continue    \n",
    "        \n",
    "        for j in range(num_critic):\n",
    "            x, y = next(data.data_generator())\n",
    "            real_a = Variable(x).to(device)\n",
    "            real_b = Variable(y).to(device)\n",
    "        \n",
    "            # Training Discriminator A with real_A batch\n",
    "            optim_dis_a.zero_grad();\n",
    "            pred_real_dis_a = dis_a(real_a).view(-1)\n",
    "            \n",
    "            # Training Discriminator B with real_B batch\n",
    "            optim_dis_b.zero_grad();\n",
    "            pred_real_dis_b = dis_b(real_b).view(-1)\n",
    "            \n",
    "            # Training Discriminator B with fake_B batch of Generator A\n",
    "            fake_b = gen_a(real_a)\n",
    "            pred_fake_dis_b = dis_b(fake_b.detach()).view(-1)\n",
    "            \n",
    "            # Training Discriminator A with fake_A batch of Generator B\n",
    "            fake_a = gen_b(real_b)\n",
    "            pred_fake_dis_a = dis_a(fake_a.detach()).view(-1)\n",
    "            \n",
    "            # Update params of Discriminator A and B\n",
    "            err_dis_a = pred_fake_dis_a.mean() - pred_real_dis_a.mean() + gradient_penalty(real_a.data, fake_a.data, dis_a)\n",
    "            err_dis_a.backward()\n",
    "            optim_dis_a.step()\n",
    "            err_dis_b = pred_fake_dis_b.mean() - pred_real_dis_b.mean() + gradient_penalty(real_b.data, fake_b.data, dis_b)\n",
    "            err_dis_b.backward()\n",
    "            optim_dis_b.step()\n",
    "            \n",
    "        # Train and update Generator A based on Discriminator B's prediction\n",
    "        x, y = next(data.data_generator())\n",
    "        real_a = Variable(x).to(device)\n",
    "        real_b = Variable(y).to(device)\n",
    "        \n",
    "        optim_gen_a.zero_grad()\n",
    "        fake_b_gen = gen_a(fake_a)\n",
    "        pred_out_dis_b = dis_b(fake_b_gen).view(-1)\n",
    "        err_gen_a_pred = -pred_out_dis_b.mean()\n",
    "        err_gen_a_pixel = criterion_pixel(fake_b_gen, real_b)\n",
    "        err_gen_a = err_gen_a_pred + err_gen_a_pixel\n",
    "        err_gen_a.backward()\n",
    "        optim_gen_a.step()\n",
    "        \n",
    "        # Train and update Generator B based on Discriminator A's prediction\n",
    "        optim_gen_b.zero_grad()\n",
    "        fake_a_gen = gen_b(fake_b)\n",
    "        pred_out_dis_a = dis_a(fake_a_gen).view(-1)\n",
    "        err_gen_b_pred = -pred_out_dis_a.mean()\n",
    "        err_gen_b_pixel = criterion_pixel(fake_a_gen, real_a)\n",
    "        err_gen_b = err_gen_b_pred + err_gen_b_pixel\n",
    "        err_gen_b.backward()\n",
    "        optim_gen_b.step()\n",
    "        \n",
    "        # Print statistics and save checkpoints\n",
    "        print(\"\\r[Epoch %d/%d] [Batch %d/%d] [D_A loss: %f] [D_B loss: %f] [G_A loss: %f, G_B loss: %f]\" %\n",
    "                                                        (epoch, num_epochs,\n",
    "                                                        i, num_images//batch_size,\n",
    "                                                        err_dis_a.item(), err_dis_b.item(), \n",
    "                                                        err_gen_a.item(), err_gen_b.item()))\n",
    "\n",
    "        if i % sample_interval == 0:\n",
    "            img_sample = torch.cat((real_a.data, fake_a.data, real_b.data, fake_b.data), -2)\n",
    "            save_image(img_sample, proj_root + 'saved_images/dual_wgans2/%d_%d.png' % (epoch, i), nrow=5, normalize=True)\n",
    "\n",
    "            torch.save(gen_a.state_dict(), proj_root + 'saved_models/dual_wgans2/generator_a.pth')\n",
    "            torch.save(gen_b.state_dict(), proj_root + 'saved_models/dual_wgans2/generator_b.pth')\n",
    "            torch.save(dis_a.state_dict(), proj_root + 'saved_models/dual_wgans2/discriminator_a.pth')\n",
    "            torch.save(dis_b.state_dict(), proj_root + 'saved_models/dual_wgans2/discriminator_b.pth')\n",
    "            e_tracker.write(epoch, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
