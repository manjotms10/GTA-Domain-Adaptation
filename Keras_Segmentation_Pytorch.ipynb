{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import itertools\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torchvision import datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for project\n",
    "proj_root = \"\"\n",
    "\n",
    "# Root directory for dataset\n",
    "data_root = \"/datasets/home/73/673/h6gupta/Project/Sematic_Segmentation/cityscapes/\"\n",
    "\n",
    "# Number of images in the directory\n",
    "num_images = 2975\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 16\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this size using a transformer.\n",
    "image_size = 256\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = torch.cuda.device_count()\n",
    "\n",
    "# Device to run on\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Parameters:\n",
    "        \n",
    "        '''\n",
    "        self.device = device\n",
    "        self.data_path = data_root\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.train_names = glob.glob(self.data_path + 'trainA/*')\n",
    "        self.names = [self.train_names[i].split('/')[-1] for i in range(len(self.train_names))]\n",
    "        self.data_transforms = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.Resize(image_size),\n",
    "                torchvision.transforms.CenterCrop(image_size),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "#                 torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "    \n",
    "    def image_loader(self, image_name):\n",
    "        \"\"\"load image, returns cuda tensor\"\"\"\n",
    "        image = Image.open(image_name)\n",
    "        image = self.data_transforms(image).float()\n",
    "        image = torch.autograd.Variable(image, requires_grad=False)\n",
    "        image = image.unsqueeze(0)  # this is for VGG, may not be needed for ResNet\n",
    "        return image[0].to(self.device)  # assumes that you're using GPU\n",
    "\n",
    "    def show(self, img):\n",
    "        npimg = img.cpu().detach().numpy()\n",
    "        npimg = np.transpose(npimg, (1,2,0))\n",
    "        if npimg.shape[2] == 3:\n",
    "            plt.imshow(npimg)\n",
    "        else:\n",
    "            plt.imshow(npimg[:,:,0], cmap='gray')\n",
    "            \n",
    "    def imshow(self, img):\n",
    "        img = img / 2 + 0.5     # unnormalize\n",
    "        npimg = img.cpu().detach().numpy()\n",
    "        plt.figure(figsize = (10,2))\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)), aspect='auto')\n",
    "\n",
    "    def data_generator(self):\n",
    "        root = self.data_path\n",
    "        batch_size = self.batch_size\n",
    "        \n",
    "        images_dir = root + 'trainA/'\n",
    "        labels_dir = root + 'trainB/'\n",
    "\n",
    "        while True:\n",
    "            x, y = [], []\n",
    "            idx = np.random.choice(self.names, batch_size)\n",
    "            for i in range(idx.shape[0]):\n",
    "                x.append(self.image_loader(images_dir + idx[i]))\n",
    "                y.append(self.image_loader(labels_dir + idx[i]))\n",
    "            yield torch.stack(x), torch.stack(y)\n",
    "            \n",
    "data = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegNet(nn.Module):\n",
    "    def __init__(self,input_nbr,label_nbr):\n",
    "        super(SegNet, self).__init__()\n",
    "\n",
    "        batchNorm_momentum = 0.1\n",
    "\n",
    "        self.conv11 = nn.Conv2d(input_nbr, 64, kernel_size=3, padding=1)\n",
    "        self.bn11 = nn.BatchNorm2d(64, momentum= batchNorm_momentum)\n",
    "        self.conv12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn12 = nn.BatchNorm2d(64, momentum= batchNorm_momentum)\n",
    "\n",
    "        self.conv21 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn21 = nn.BatchNorm2d(128, momentum= batchNorm_momentum)\n",
    "        self.conv22 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn22 = nn.BatchNorm2d(128, momentum= batchNorm_momentum)\n",
    "\n",
    "        self.conv31 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn31 = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n",
    "        self.conv32 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn32 = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n",
    "        self.conv33 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn33 = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n",
    "\n",
    "        self.conv41 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn41 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n",
    "        self.conv42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn42 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n",
    "        self.conv43 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn43 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n",
    "\n",
    "        self.conv51 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn51 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n",
    "        self.conv52 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn52 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n",
    "        self.conv53 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn53 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n",
    "\n",
    "        self.conv53d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn53d = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n",
    "        self.conv52d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn52d = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n",
    "        self.conv51d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn51d = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n",
    "\n",
    "        self.conv43d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn43d = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n",
    "        self.conv42d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn42d = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n",
    "        self.conv41d = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.bn41d = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n",
    "\n",
    "        self.conv33d = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn33d = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n",
    "        self.conv32d = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn32d = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n",
    "        self.conv31d = nn.Conv2d(256,  128, kernel_size=3, padding=1)\n",
    "        self.bn31d = nn.BatchNorm2d(128, momentum= batchNorm_momentum)\n",
    "\n",
    "        self.conv22d = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn22d = nn.BatchNorm2d(128, momentum= batchNorm_momentum)\n",
    "        self.conv21d = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.bn21d = nn.BatchNorm2d(64, momentum= batchNorm_momentum)\n",
    "\n",
    "        self.conv12d = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn12d = nn.BatchNorm2d(64, momentum= batchNorm_momentum)\n",
    "        self.conv11d = nn.Conv2d(64, label_nbr, kernel_size=3, padding=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Stage 1\n",
    "        x11 = F.relu(self.bn11(self.conv11(x)))\n",
    "        x12 = F.relu(self.bn12(self.conv12(x11)))\n",
    "        x1p, id1 = F.max_pool2d(x12,kernel_size=2, stride=2,return_indices=True)\n",
    "\n",
    "        # Stage 2\n",
    "        x21 = F.relu(self.bn21(self.conv21(x1p)))\n",
    "        x22 = F.relu(self.bn22(self.conv22(x21)))\n",
    "        x2p, id2 = F.max_pool2d(x22,kernel_size=2, stride=2,return_indices=True)\n",
    "\n",
    "        # Stage 3\n",
    "        x31 = F.relu(self.bn31(self.conv31(x2p)))\n",
    "        x32 = F.relu(self.bn32(self.conv32(x31)))\n",
    "        x33 = F.relu(self.bn33(self.conv33(x32)))\n",
    "        x3p, id3 = F.max_pool2d(x33,kernel_size=2, stride=2,return_indices=True)\n",
    "\n",
    "        # Stage 4\n",
    "        x41 = F.relu(self.bn41(self.conv41(x3p)))\n",
    "        x42 = F.relu(self.bn42(self.conv42(x41)))\n",
    "        x43 = F.relu(self.bn43(self.conv43(x42)))\n",
    "        x4p, id4 = F.max_pool2d(x43,kernel_size=2, stride=2,return_indices=True)\n",
    "\n",
    "        # Stage 5\n",
    "        x51 = F.relu(self.bn51(self.conv51(x4p)))\n",
    "        x52 = F.relu(self.bn52(self.conv52(x51)))\n",
    "        x53 = F.relu(self.bn53(self.conv53(x52)))\n",
    "        x5p, id5 = F.max_pool2d(x53,kernel_size=2, stride=2,return_indices=True)\n",
    "\n",
    "\n",
    "        # Stage 5d\n",
    "        x5d = F.max_unpool2d(x5p, id5, kernel_size=2, stride=2)\n",
    "        x53d = F.relu(self.bn53d(self.conv53d(x5d)))\n",
    "        x52d = F.relu(self.bn52d(self.conv52d(x53d)))\n",
    "        x51d = F.relu(self.bn51d(self.conv51d(x52d)))\n",
    "\n",
    "        # Stage 4d\n",
    "        x4d = F.max_unpool2d(x51d, id4, kernel_size=2, stride=2)\n",
    "        x43d = F.relu(self.bn43d(self.conv43d(x4d)))\n",
    "        x42d = F.relu(self.bn42d(self.conv42d(x43d)))\n",
    "        x41d = F.relu(self.bn41d(self.conv41d(x42d)))\n",
    "\n",
    "        # Stage 3d\n",
    "        x3d = F.max_unpool2d(x41d, id3, kernel_size=2, stride=2)\n",
    "        x33d = F.relu(self.bn33d(self.conv33d(x3d)))\n",
    "        x32d = F.relu(self.bn32d(self.conv32d(x33d)))\n",
    "        x31d = F.relu(self.bn31d(self.conv31d(x32d)))\n",
    "\n",
    "        # Stage 2d\n",
    "        x2d = F.max_unpool2d(x31d, id2, kernel_size=2, stride=2)\n",
    "        x22d = F.relu(self.bn22d(self.conv22d(x2d)))\n",
    "        x21d = F.relu(self.bn21d(self.conv21d(x22d)))\n",
    "\n",
    "        # Stage 1d\n",
    "        x1d = F.max_unpool2d(x21d, id1, kernel_size=2, stride=2)\n",
    "        x12d = F.relu(self.bn12d(self.conv12d(x1d)))\n",
    "        x11d = self.conv11d(x12d)\n",
    "\n",
    "        return x11d\n",
    "    \n",
    "#     def load_weights(self, model_path):\n",
    "#         s_dict = self.state_dict()# create a copy of the state dict\n",
    "#         th = torch.load(model_path).state_dict() # load the weigths\n",
    "#         # for name in th:\n",
    "#             # s_dict[corresp_name[name]] = th[name]\n",
    "#         self.load_state_dict(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        # Convolution layers\n",
    "        \n",
    "        # input is (nc) x 256 x 256\n",
    "        self.conv1 = nn.Conv2d(nc, ngf, 4, 2, 1, bias=True)\n",
    "        self.lr1 = nn.LeakyReLU(inplace=True)\n",
    "        # state size. (ngf) x 128 x 128\n",
    "        self.conv2 = nn.Conv2d(ngf, ngf*2, 4, 2, 1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(ngf*2)\n",
    "        self.lr2 = nn.LeakyReLU(inplace=True)\n",
    "        # state size. (ngf*2) x 64 x 64\n",
    "        self.conv3 = nn.Conv2d(ngf*2, ngf*4, 4, 2, 1, bias=True)\n",
    "        self.bn3 = nn.BatchNorm2d(ngf*4)\n",
    "        self.lr3 = nn.LeakyReLU(inplace=True)\n",
    "        # state size. (ngf*4) x 32 x 32\n",
    "        self.conv4 = nn.Conv2d(ngf*4, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.bn4 = nn.BatchNorm2d(ngf*8)\n",
    "        self.lr4 = nn.LeakyReLU(inplace=True)\n",
    "        # state size. (ngf*8) x 16 x 16\n",
    "        self.conv5 = nn.Conv2d(ngf*8, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.bn5 = nn.BatchNorm2d(ngf*8)\n",
    "        self.lr5 = nn.LeakyReLU(inplace=True)\n",
    "        # state size. (ngf*8) x 8 x 8\n",
    "        self.conv6 = nn.Conv2d(ngf*8, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.bn6 = nn.BatchNorm2d(ngf*8)\n",
    "        self.lr6 = nn.LeakyReLU(inplace=True)\n",
    "        # state size. (ngf*8) x 4 x 4\n",
    "        self.conv7 = nn.Conv2d(ngf*8, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.bn7 = nn.BatchNorm2d(ngf*8)\n",
    "        self.lr7 = nn.LeakyReLU(inplace=True)\n",
    "        # state size. (ngf*8) x 2 x 2\n",
    "        self.conv8 = nn.Conv2d(ngf*8, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.bn8 = nn.BatchNorm2d(ngf*8)\n",
    "        self.r8 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Transpose Convolutional Layers\n",
    "        \n",
    "        # input is (ngf*8) x 1 x 1\n",
    "        self.tr_conv1 = nn.ConvTranspose2d(ngf*8, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.tr_bn1 = nn.BatchNorm2d(ngf*8)\n",
    "        self.tr_r1 = nn.ReLU(inplace=True)\n",
    "        # state size. (ngf*8)*2 x 2 x 2\n",
    "        self.tr_conv2 = nn.ConvTranspose2d((ngf*8)*2, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.tr_bn2 = nn.BatchNorm2d(ngf*8)\n",
    "        self.tr_r2 = nn.ReLU(inplace=True)\n",
    "        # state size. (ngf*8)*2 x 4 x 4\n",
    "        self.tr_conv3 = nn.ConvTranspose2d((ngf*8)*2, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.tr_bn3 = nn.BatchNorm2d(ngf*8)\n",
    "        self.tr_r3 = nn.ReLU(inplace=True)\n",
    "        # state size. (ngf*8)*2 x 8 x 8\n",
    "        self.tr_conv4 = nn.ConvTranspose2d((ngf*8)*2, ngf*8, 4, 2, 1, bias=True)\n",
    "        self.tr_bn4 = nn.BatchNorm2d(ngf*8)\n",
    "        self.tr_r4 = nn.ReLU(inplace=True)\n",
    "        # state size. (ngf*8)*2 x 16 x 16\n",
    "        self.tr_conv5 = nn.ConvTranspose2d((ngf*8)*2, ngf*4, 4, 2, 1, bias=True)\n",
    "        self.tr_bn5 = nn.BatchNorm2d(ngf*4)\n",
    "        self.tr_r5 = nn.ReLU(inplace=True)\n",
    "        # state size. (ngf*4)*2 x 32 x 32\n",
    "        self.tr_conv6 = nn.ConvTranspose2d((ngf*4)*2, ngf*2, 4, 2, 1, bias=True)\n",
    "        self.tr_bn6 = nn.BatchNorm2d(ngf*2)\n",
    "        self.tr_r6 = nn.ReLU(inplace=True)\n",
    "        # state size. (ngf*2)*2 x 64 x 64\n",
    "        self.tr_conv7 = nn.ConvTranspose2d((ngf*2)*2, ngf, 4, 2, 1, bias=True)\n",
    "        self.tr_bn7 = nn.BatchNorm2d(ngf)\n",
    "        self.tr_r7 = nn.ReLU(inplace=True)\n",
    "        # state size. (ngf)*2 x 128 x 128\n",
    "        self.tr_conv8 = nn.ConvTranspose2d((ngf)*2, nc, 4, 2, 1, bias=True)\n",
    "        self.out = nn.Tanh()\n",
    "        # state size. (nc) x 256 x 256\n",
    "    \n",
    "    def forward(self, x):\n",
    "        c1 = self.conv1(x)\n",
    "        c2 = self.bn2(self.conv2(self.lr1(c1)))\n",
    "        c3 = self.bn3(self.conv3(self.lr2(c2)))\n",
    "        c4 = self.bn4(self.conv4(self.lr3(c3)))\n",
    "        c5 = self.bn5(self.conv5(self.lr4(c4)))\n",
    "        c6 = self.bn6(self.conv6(self.lr5(c5)))\n",
    "        c7 = self.bn7(self.conv7(self.lr6(c6)))\n",
    "        c8 = self.bn8(self.conv8(self.lr7(c7)))\n",
    "        \n",
    "        t1 = self.tr_bn1(self.tr_conv1(self.r8(c8)))\n",
    "        t1 = torch.cat((t1, c7), dim=1)\n",
    "        t2 = self.tr_bn2(self.tr_conv2(self.tr_r1(t1)))\n",
    "        t2 = torch.cat((t2, c6), dim=1)\n",
    "        t3 = self.tr_bn3(self.tr_conv3(self.tr_r2(t2)))\n",
    "        t3 = torch.cat((t3, c5), dim=1)\n",
    "        t4 = self.tr_bn4(self.tr_conv4(self.tr_r3(t3)))\n",
    "        t4 = torch.cat((t4, c4), dim=1)\n",
    "        t5 = self.tr_bn5(self.tr_conv5(self.tr_r4(t4)))\n",
    "        t5 = torch.cat((t5, c3), dim=1)\n",
    "        t6 = self.tr_bn6(self.tr_conv6(self.tr_r5(t5)))\n",
    "        t6 = torch.cat((t6, c2), dim=1)\n",
    "        t7 = self.tr_bn7(self.tr_conv7(self.tr_r6(t6)))\n",
    "        t7 = torch.cat((t7, c1), dim=1)\n",
    "        t8 = self.tr_conv8(self.tr_r7(t7))\n",
    "        t8 = self.out(t8)\n",
    "        return t8\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Generator().to(device)\n",
    "# model = SegNet(3,3).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "# model.load_state_dict(torch.load(\"vgg16-00b39a1b.pth\"))\n",
    "# model.load_weights(\"vgg16-00b39a1b.pth\")\n",
    "# model.to(device)\n",
    "# Loss = nn.BCELoss()\n",
    "# Loss = nn.CrossEntropyLoss()\n",
    "Loss = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(label, output):\n",
    "    smooth = 1e-5\n",
    "    label = torch.round(label.view(-1))\n",
    "    output = torch.round(output.view(-1))\n",
    "    isct = torch.sum(torch.mul(label,output))\n",
    "    return (2 *(isct) / (torch.sum(label) + torch.sum(output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0, 100):\n",
    "    for i in range(num_images // batch_size):\n",
    "        x, y = next(data.data_generator())\n",
    "        img = Variable(x).to(device)\n",
    "        label = Variable(y).to(device)\n",
    "        predicted = model(img[0:12,:])\n",
    "#         print(predicted.size())\n",
    "#         print(type(label))\n",
    "        loss = Loss(predicted,label[0:12,:])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(i%50 == 0):\n",
    "            val = model(img[12:16,:])\n",
    "            val_loss = Loss(val,label[12:16,:])\n",
    "            print ('Epoch: {}, Batch: {}, train_Loss: {}, val_loss: {}, IOU_Train: {}, IOU_Val: {}' \n",
    "                   .format(epoch+1, i, loss.item(),val_loss.item(),dice_coef(label[0:12,:],predicted), dice_coef(label[12:16,:],val)))\n",
    "            img_sample = torch.cat((label[14,:].data, val[2,:].data),-1)\n",
    "#             print(type(img_sample))\n",
    "            save_image(img_sample, '/datasets/home/73/673/h6gupta/Project/Sematic_Segmentation/cross_entropy/output/%d.png' % (epoch),nrow=2, normalize=True)\n",
    "    torch.save(model.state_dict(), '/datasets/home/73/673/h6gupta/Project/Sematic_Segmentation/cross_entropy/Model_Semantic.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
