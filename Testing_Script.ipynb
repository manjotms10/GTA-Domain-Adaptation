{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import itertools\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torchvision import datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "import networks\n",
    "import data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl_file = open(\"numpyModel.pkl\", \"rb\")\n",
    "# data_ = pickle.load(pkl_file, encoding=\"latin1\")\n",
    "# dt = {}\n",
    "# h = { \"bn2.num_batches_tracked\", \"bn3.num_batches_tracked\", \"bn4.num_batches_tracked\", \"bn5.num_batches_tracked\", \"bn6.num_batches_tracked\", \"bn7.num_batches_tracked\", \"bn8.num_batches_tracked\", \"tr_bn1.num_batches_tracked\", \"tr_bn2.num_batches_tracked\", \"tr_bn3.num_batches_tracked\", \"tr_bn4.num_batches_tracked\", \"tr_bn5.num_batches_tracked\", \"tr_bn6.num_batches_tracked\", \"tr_bn7.num_batches_tracked\"}\n",
    "# for key, value in data_.items():\n",
    "#     if key not in h:\n",
    "#         dt[key] = torch.from_numpy(value) \n",
    "# torch.save(dt, \"Model_Semantic_New.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "data_root = \"/datasets/home/73/673/h6gupta/Project/Sematic_Segmentation/gt_for_cv/\"\n",
    "# Number of images in the directory\n",
    "num_images = 800\n",
    "# Batch size during training\n",
    "batch_size = 32\n",
    "# Spatial size of training images. All images will be resized to this size using a transformer.\n",
    "image_size = 256\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = torch.cuda.device_count()\n",
    "# Device to run on\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def dice_coef(label, output):\n",
    "    label = torch.round(label.view(-1))\n",
    "    output = torch.round(output.view(-1))\n",
    "    isct = torch.sum(torch.mul(label,output))\n",
    "    return (2 *(isct) / (torch.sum(label) + torch.sum(output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_loader.DataLoader(data_root, image_size, batch_size, train = False, folder_A = \"images/\", folder_B = \"labels/\")\n",
    "GTA2CITY = networks.GeneratorUNet().to(device)\n",
    "GTA2CITY.load_state_dict(torch.load('../../../../saved_models/dual_gans_semi/generator_b.pth_8_1462.pth'))\n",
    "Semantic = networks.GeneratorUNet().to(device)\n",
    "Semantic.load_state_dict(torch.load('Model_Semantic_Final.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 IOU: 0.7283569574356079\n",
      "1 IOU: 0.7407826781272888\n",
      "2 IOU: 0.7578699588775635\n",
      "3 IOU: 0.7619194984436035\n",
      "4 IOU: 0.7237129807472229\n",
      "5 IOU: 0.7397133708000183\n",
      "6 IOU: 0.7300137281417847\n",
      "7 IOU: 0.7616379261016846\n",
      "8 IOU: 0.7168512344360352\n",
      "9 IOU: 0.7343743443489075\n",
      "10 IOU: 0.775820791721344\n",
      "11 IOU: 0.7310453653335571\n",
      "12 IOU: 0.7310613393783569\n",
      "13 IOU: 0.7451351284980774\n",
      "14 IOU: 0.7333023548126221\n",
      "15 IOU: 0.7511754631996155\n",
      "16 IOU: 0.7480047941207886\n",
      "17 IOU: 0.7672289609909058\n",
      "18 IOU: 0.750859260559082\n",
      "19 IOU: 0.7370699048042297\n",
      "20 IOU: 0.7321837544441223\n",
      "21 IOU: 0.7552876472473145\n",
      "22 IOU: 0.7643251419067383\n",
      "23 IOU: 0.7260404825210571\n",
      "24 IOU: 0.7571142911911011\n"
     ]
    }
   ],
   "source": [
    "iou = []\n",
    "for i in range(num_images // batch_size):\n",
    "    x,y = next(data.data_generator(0, train = False))\n",
    "    img = Variable(x, requires_grad = False).to(device)\n",
    "    lbl = Variable(y, requires_grad = False).to(device)\n",
    "    \n",
    "    img = (img - torch.min(img))/(torch.max(img - torch.min(img)))\n",
    "    lbl = (lbl - torch.min(lbl))/(torch.max(lbl - torch.min(lbl)))\n",
    "    \n",
    "    cityscapes = GTA2CITY(img)\n",
    "    cityscapes = (cityscapes - torch.min(cityscapes))/(torch.max(cityscapes - torch.min(cityscapes)))\n",
    "    \n",
    "    semantic_sample = Semantic(cityscapes)\n",
    "    semantic_sample = (semantic_sample - torch.min(semantic_sample))/(torch.max(semantic_sample - torch.min(semantic_sample)))\n",
    "    \n",
    "    print (i, 'IOU: {}'.format(dice_coef(lbl, semantic_sample)))\n",
    "    iou.append(dice_coef(lbl, semantic_sample).cpu().detach().numpy())\n",
    "    for j in range(img.size()[0]):\n",
    "        img_sample = torch.cat((img[j,:,:,:].data,lbl[j,:,:,:].data, cityscapes[j,:,:,:], semantic_sample[j,:,:,:]),-1)\n",
    "        save_image(img_sample, '/datasets/home/73/673/h6gupta/Project/Sematic_Segmentation/MSELOSS/Final_Testing/outputs/%d.png' % ((batch_size*i) + j), nrow=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean IOU: 0.7240797424316406\n"
     ]
    }
   ],
   "source": [
    "print ('Mean IOU: {}'.format(np.sum(np.asarray(iou))/(i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17722058296203613\n"
     ]
    }
   ],
   "source": [
    "# print(semantic_sample.size())\n",
    "Loss = torch.nn.L1Loss()\n",
    "loss = Loss(semantic_sample,lbl)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
